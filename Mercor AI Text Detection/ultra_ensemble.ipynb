{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5aa7643",
   "metadata": {},
   "source": [
    "# Mercor AI Text Detection â€” Ultra Ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2448f64f",
   "metadata": {},
   "source": [
    "This notebook builds a stacked ensemble for the Mercor AI Text Detection competition.It combines high-signal character/word TF-IDF models with stylometric gradient-boosting andrandom-forest learners, then calibrates and applies a stylistic confidence adjustment beforeproducing the final submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd4a7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.base import clone\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "\n",
    "RANDOM_STATE = 128\n",
    "np.random.seed(RANDOM_STATE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1d8b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = Path('Data')\n",
    "if not DATA_DIR.exists():\n",
    "    DATA_DIR = Path('data')\n",
    "\n",
    "train_df = pd.read_csv(DATA_DIR / 'train.csv')\n",
    "test_df = pd.read_csv(DATA_DIR / 'test.csv')\n",
    "y = train_df['is_cheating'].values\n",
    "\n",
    "print(f'Train shape: {train_df.shape}')\n",
    "print(f'Test shape: {test_df.shape}')\n",
    "print()\n",
    "print('Label distribution:')\n",
    "print(train_df['is_cheating'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d523b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_stylometric(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    text = df['answer'].fillna('')\n",
    "    topic = df['topic'].fillna('')\n",
    "\n",
    "    words = text.str.split()\n",
    "    word_counts = words.apply(len)\n",
    "    unique_counts = words.apply(lambda tokens: len(set(tokens)) if tokens else 0)\n",
    "    char_counts = text.str.len()\n",
    "    sentence_counts = text.str.count(r'[.!?]') + 1\n",
    "    comma_counts = text.str.count(',')\n",
    "    punctuation_counts = text.str.count(r'[.,;:!?]')\n",
    "    uppercase_counts = text.str.count(r'[A-Z]')\n",
    "    digit_counts = text.str.count(r'[0-9]')\n",
    "    newline_counts = text.str.count(chr(10))\n",
    "\n",
    "    connector_terms = [\n",
    "        'in conclusion', 'furthermore', 'moreover', 'additionally', 'however',\n",
    "        'therefore', 'thus', 'consequently', 'as a result', 'for example',\n",
    "        'for instance', 'overall', 'in summary'\n",
    "    ]\n",
    "    formal_terms = [\n",
    "        'utilize', 'methodology', 'paradigm', 'robust', 'demonstrate',\n",
    "        'optimise', 'framework', 'leverage', 'strategic', 'comprehensive'\n",
    "    ]\n",
    "    passive_terms = ['is made', 'was made', 'is given', 'was given', 'is shown', 'was shown']\n",
    "\n",
    "    df_feat = pd.DataFrame({\n",
    "        'char_count': char_counts,\n",
    "        'word_count': word_counts,\n",
    "        'unique_words': unique_counts,\n",
    "        'avg_word_len': char_counts / (word_counts + 1),\n",
    "        'sentence_count': sentence_counts,\n",
    "        'avg_sentence_len': word_counts / (sentence_counts + 1),\n",
    "        'comma_count': comma_counts,\n",
    "        'punctuation_count': punctuation_counts,\n",
    "        'uppercase_count': uppercase_counts,\n",
    "        'digit_count': digit_counts,\n",
    "        'newline_count': newline_counts,\n",
    "        'connector_hits': text.apply(lambda x: sum(term in x.lower() for term in connector_terms)),\n",
    "        'formal_hits': text.apply(lambda x: sum(term in x.lower() for term in formal_terms)),\n",
    "        'passive_hits': text.apply(lambda x: sum(term in x.lower() for term in passive_terms)),\n",
    "        'quote_count': text.str.count('\"'),\n",
    "        'colon_count': text.str.count(':'),\n",
    "        'semicolon_count': text.str.count(';'),\n",
    "        'exclamation_count': text.str.count('!'),\n",
    "        'question_count': text.str.count(r'\\?'),\n",
    "        'topic_char': topic.str.len(),\n",
    "        'topic_words': topic.str.split().apply(len),\n",
    "        'topic_caps': topic.str.count(r'[A-Z]'),\n",
    "    })\n",
    "\n",
    "    df_feat['unique_ratio'] = df_feat['unique_words'] / (df_feat['word_count'] + 1)\n",
    "    df_feat['punct_ratio'] = df_feat['punctuation_count'] / (df_feat['char_count'] + 1)\n",
    "    df_feat['upper_ratio'] = df_feat['uppercase_count'] / (df_feat['char_count'] + 1)\n",
    "    df_feat['digit_ratio'] = df_feat['digit_count'] / (df_feat['char_count'] + 1)\n",
    "    df_feat['topic_char_per_word'] = df_feat['topic_char'] / (df_feat['topic_words'] + 1)\n",
    "    return df_feat.fillna(0)\n",
    "\n",
    "stylometric_train = build_stylometric(train_df)\n",
    "stylometric_test = build_stylometric(test_df)\n",
    "print('Stylometric feature shape:', stylometric_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292a6199",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_MODELS = [\n",
    "    {\n",
    "        'name': 'lr_char_3_7',\n",
    "        'type': 'tfidf',\n",
    "        'params': {\n",
    "            'vectorizer': TfidfVectorizer(analyzer='char', ngram_range=(3, 7), min_df=2, max_df=0.99),\n",
    "            'estimator': LogisticRegression(max_iter=6000, C=2.5, class_weight='balanced', solver='lbfgs'),\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        'name': 'lr_charwb_2_6',\n",
    "        'type': 'tfidf',\n",
    "        'params': {\n",
    "            'vectorizer': TfidfVectorizer(analyzer='char_wb', ngram_range=(2, 6), min_df=2, max_df=0.99),\n",
    "            'estimator': LogisticRegression(max_iter=6000, C=3.0, class_weight='balanced', solver='lbfgs'),\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        'name': 'sgd_char_2_7',\n",
    "        'type': 'tfidf',\n",
    "        'params': {\n",
    "            'vectorizer': TfidfVectorizer(analyzer='char', ngram_range=(2, 7), min_df=2, max_df=0.995),\n",
    "            'estimator': SGDClassifier(loss='log_loss', penalty='elasticnet', alpha=5e-5, l1_ratio=0.25,\n",
    "                                      max_iter=4000, tol=1e-3, class_weight='balanced',\n",
    "                                      n_iter_no_change=20, random_state=RANDOM_STATE),\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        'name': 'lr_word_1_3',\n",
    "        'type': 'tfidf',\n",
    "        'params': {\n",
    "            'vectorizer': TfidfVectorizer(analyzer='word', ngram_range=(1, 3), min_df=2, max_df=0.9, sublinear_tf=True),\n",
    "            'estimator': LogisticRegression(max_iter=5000, C=2.0, class_weight='balanced', solver='lbfgs'),\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        'name': 'gb_stylo',\n",
    "        'type': 'stylo',\n",
    "        'params': {\n",
    "            'scaler': StandardScaler(),\n",
    "            'estimator': GradientBoostingClassifier(random_state=RANDOM_STATE, n_estimators=600,\n",
    "                                                    learning_rate=0.02, max_depth=3, subsample=0.9,\n",
    "                                                    min_samples_leaf=3),\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        'name': 'rf_stylo',\n",
    "        'type': 'stylo',\n",
    "        'params': {\n",
    "            'scaler': None,\n",
    "            'estimator': RandomForestClassifier(n_estimators=400, max_depth=7, min_samples_leaf=4,\n",
    "                                                random_state=RANDOM_STATE, n_jobs=-1),\n",
    "        }\n",
    "    }\n",
    "]\n",
    "print('Configured base models:', [spec['name'] for spec in BASE_MODELS])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c816ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_models = len(BASE_MODELS)\n",
    "skf = StratifiedKFold(n_splits=6, shuffle=True, random_state=RANDOM_STATE)\n",
    "oof_matrix = np.zeros((len(train_df), n_models), dtype=np.float32)\n",
    "test_matrix = np.zeros((len(test_df), n_models), dtype=np.float32)\n",
    "\n",
    "for fold, (train_idx, valid_idx) in enumerate(skf.split(train_df, y), start=1):\n",
    "    y_tr = y[train_idx]\n",
    "    ans_tr = train_df['answer'].iloc[train_idx]\n",
    "    ans_val = train_df['answer'].iloc[valid_idx]\n",
    "\n",
    "    for model_pos, spec in enumerate(BASE_MODELS):\n",
    "        if spec['type'] == 'tfidf':\n",
    "            vectorizer = clone(spec['params']['vectorizer'])\n",
    "            estimator = clone(spec['params']['estimator'])\n",
    "            X_tr = vectorizer.fit_transform(ans_tr)\n",
    "            X_val = vectorizer.transform(ans_val)\n",
    "            estimator.fit(X_tr, y_tr)\n",
    "            oof_matrix[valid_idx, model_pos] = estimator.predict_proba(X_val)[:, 1]\n",
    "            X_test = vectorizer.transform(test_df['answer'])\n",
    "            test_matrix[:, model_pos] += estimator.predict_proba(X_test)[:, 1] / skf.n_splits\n",
    "        else:\n",
    "            features_tr = stylometric_train.iloc[train_idx]\n",
    "            features_val = stylometric_train.iloc[valid_idx]\n",
    "            estimator = clone(spec['params']['estimator'])\n",
    "            scaler = spec['params']['scaler']\n",
    "            if scaler is not None:\n",
    "                scaler = clone(scaler)\n",
    "                X_tr = scaler.fit_transform(features_tr)\n",
    "                X_val = scaler.transform(features_val)\n",
    "                X_test = scaler.transform(stylometric_test)\n",
    "            else:\n",
    "                X_tr = features_tr.values\n",
    "                X_val = features_val.values\n",
    "                X_test = stylometric_test.values\n",
    "            estimator.fit(X_tr, y_tr)\n",
    "            oof_matrix[valid_idx, model_pos] = estimator.predict_proba(X_val)[:, 1]\n",
    "            test_matrix[:, model_pos] += estimator.predict_proba(X_test)[:, 1] / skf.n_splits\n",
    "        gc.collect()\n",
    "\n",
    "    blend_auc = roc_auc_score(y[valid_idx], oof_matrix[valid_idx].mean(axis=1))\n",
    "    print(f'Fold {fold} complete â€” blended fold AUC: {blend_auc:.6f}')\n",
    "\n",
    "base_scores = {\n",
    "    spec['name']: roc_auc_score(y, oof_matrix[:, idx])\n",
    "    for idx, spec in enumerate(BASE_MODELS)\n",
    "}\n",
    "print()\n",
    "print('OOF AUC per base model:')\n",
    "for name, score in base_scores.items():\n",
    "    print(f'  {name}: {score:.6f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf8b969",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_model = LogisticRegression(max_iter=6000, solver='lbfgs', random_state=RANDOM_STATE)\n",
    "meta_model.fit(oof_matrix, y)\n",
    "meta_oof = meta_model.predict_proba(oof_matrix)[:, 1]\n",
    "meta_test = meta_model.predict_proba(test_matrix)[:, 1]\n",
    "\n",
    "meta_auc = roc_auc_score(y, meta_oof)\n",
    "print(f'Level-2 logistic meta AUC: {meta_auc:.6f}')\n",
    "\n",
    "calibrator = IsotonicRegression(out_of_bounds='clip')\n",
    "calibrator.fit(meta_oof, y)\n",
    "calibrated_oof = calibrator.transform(meta_oof)\n",
    "calibrated_test = calibrator.transform(meta_test)\n",
    "print(f'Calibrated OOF AUC: {roc_auc_score(y, calibrated_oof):.6f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5f85a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def confidence_adjustment(raw_scores: np.ndarray, stylometric_df: pd.DataFrame) -> np.ndarray:\n",
    "    scores = np.asarray(raw_scores, dtype=np.float64)\n",
    "    adjusted = scores.copy()\n",
    "\n",
    "    mid_band = (adjusted > 0.38) & (adjusted < 0.62)\n",
    "    if mid_band.any():\n",
    "        subset = stylometric_df.loc[mid_band]\n",
    "        delta = np.zeros(len(subset), dtype=np.float64)\n",
    "        delta += 0.08 * (subset['avg_word_len'].values > 5.7)\n",
    "        delta += 0.05 * (subset['unique_ratio'].values < 0.58)\n",
    "        delta -= 0.05 * (subset['unique_ratio'].values > 0.70)\n",
    "        delta += 0.04 * (subset['connector_hits'].values >= 1)\n",
    "        delta += 0.03 * (subset['passive_hits'].values >= 1)\n",
    "        adjusted[mid_band] = np.clip(adjusted[mid_band] + delta, 0, 1)\n",
    "\n",
    "    heavy_formal = stylometric_df['formal_hits'].values >= 2\n",
    "    adjusted[heavy_formal] = np.clip(adjusted[heavy_formal] + 0.06, 0, 1)\n",
    "\n",
    "    overt_creative = (stylometric_df['unique_ratio'].values > 0.75) & (stylometric_df['avg_word_len'].values < 5.2)\n",
    "    adjusted[overt_creative] = np.clip(adjusted[overt_creative] - 0.07, 0, 1)\n",
    "\n",
    "    return np.clip(adjusted, 0.001, 0.999)\n",
    "\n",
    "adjusted_train = confidence_adjustment(calibrated_oof, stylometric_train)\n",
    "train_accuracy = np.mean((adjusted_train >= 0.5) == y)\n",
    "train_margin = np.min(np.abs(adjusted_train - y))\n",
    "print(f'Adjusted train accuracy: {train_accuracy:.6f}')\n",
    "print(f'Minimum confidence margin: {train_margin:.6f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d76615",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_test_matrix = np.zeros((len(test_df), len(BASE_MODELS)), dtype=np.float32)\n",
    "\n",
    "for model_pos, spec in enumerate(BASE_MODELS):\n",
    "    if spec['type'] == 'tfidf':\n",
    "        vectorizer = clone(spec['params']['vectorizer'])\n",
    "        estimator = clone(spec['params']['estimator'])\n",
    "        X_train_full = vectorizer.fit_transform(train_df['answer'])\n",
    "        X_test_full = vectorizer.transform(test_df['answer'])\n",
    "        estimator.fit(X_train_full, y)\n",
    "        full_test_matrix[:, model_pos] = estimator.predict_proba(X_test_full)[:, 1]\n",
    "    else:\n",
    "        estimator = clone(spec['params']['estimator'])\n",
    "        scaler = spec['params']['scaler']\n",
    "        if scaler is not None:\n",
    "            scaler = clone(scaler)\n",
    "            X_train_full = scaler.fit_transform(stylometric_train)\n",
    "            X_test_full = scaler.transform(stylometric_test)\n",
    "        else:\n",
    "            X_train_full = stylometric_train.values\n",
    "            X_test_full = stylometric_test.values\n",
    "        estimator.fit(X_train_full, y)\n",
    "        full_test_matrix[:, model_pos] = estimator.predict_proba(X_test_full)[:, 1]\n",
    "    gc.collect()\n",
    "\n",
    "meta_test_full = meta_model.predict_proba(full_test_matrix)[:, 1]\n",
    "calibrated_test_full = calibrator.transform(meta_test_full)\n",
    "adjusted_test = confidence_adjustment(calibrated_test_full, stylometric_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515ed753",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({'id': test_df['id'], 'is_cheating': adjusted_test})\n",
    "submission_path = Path('submission.csv')\n",
    "submission.to_csv(submission_path, index=False)\n",
    "print(f'Submission saved to {submission_path.resolve()}')\n",
    "print(submission.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9376044",
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnostics = pd.DataFrame({\n",
    "    'id': train_df['id'],\n",
    "    'topic': train_df['topic'],\n",
    "    'target': y,\n",
    "    'meta_oof': meta_oof,\n",
    "    'calibrated': calibrated_oof,\n",
    "    'adjusted': adjusted_train,\n",
    "})\n",
    "diagnostics['margin'] = np.abs(diagnostics['adjusted'] - diagnostics['target'])\n",
    "diagnostics.sort_values('margin', inplace=True)\n",
    "diagnostics.to_csv('training_diagnostics.csv', index=False)\n",
    "print('Training diagnostics written to training_diagnostics.csv')\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}